---
title: 'Blog 1: Tidy Models'
author: David Simbandumwe
subtitle: Value of Opinionated Frameworks
output:
  word_document:
    toc: yes
  html_document:
    toc: yes
    toc_float: yes
    theme: united
  pdf_document:
    toc: yes
editor_options:
  chunk_output_type: inline
  markdown:
    wrap: sentence
bibliography: references.bib
---


# Introduction

The Tidymodels framework is a collection of modeling and machine learning packages using Tidyverse principles. In contrast to the flexibility of R, Tidymodels is an opinionated system with an underlying solution philosophy. For this blog post, I wanted to explore the implications of the Tidymodel approach on the development effort. Tidymodels goes beyond exposing modeling capabilities and dictates a solution methodology and workflow for problem-solving. 

Opinionated systems such as Tidymodels have several benefits, including:

- Consistency - Opinionated systems promote a consistent mental model, approach, and workflow for solving problems. 
- Encapsulation of Best Practices - Opinionated frameworks provide guardrails that inherently guide developers toward best practices. Furthermore, the execution model enforces a workflow and approach to problem-solving. 
- Faster Development - Reducing the required upfront decisions and providing framework support for everyday modeling tasks accelerates the development process. The individual developers and teams do not need to write the plumbing, connectivity, or boilerplate code for basic functions. 

In the negative column, the drawbacks of opinioned systems include:

- Required Buy-In - To use the Tidymodel framework, you must buy into the authors' decisions and problem framing. Utilizing individual elements of the framework without adopting the entire solution is a difficult prospect. 
- Hidden Decisions - Standardization of the interfaces to models simplifies the execution but involves some decisions regarding default values.




# Tidymodels

The Tidymodels framework is a collection of modeling and machine learning packages. The core packages that make up the Tidymodel universe include rsample, parsnip, recipes, workflows, tune, yardstick, broom, and dials. For this blog post, I will explore how these packages impact data preparation, model definition, and the model execution workflow. 




```{r}

library(tidyverse)
library(tidymodels)
library(workflowsets)
library(dotwhisker)
library(summarytools)

```





## Tidy Data

The Tidyverse advocates for Tidy Data, a consistent representation of the model data. The data preparation step transforms data into a consistent model that adheres to the following rules:
a) Each variable must have its own column.
b) Each observation must have its own row.
c) Each value must have its own cell.

Uniform data that conforms to tidy data specifications is more consistent and easier to work with. Furthermore, the rules associated with Tidy Data enable efficient manipulation with tools in the Tidyverse such as dplyr, or ggplot2. 

<img src="tidy_data.png" width="1200" style="display: block; margin-left: auto; margin-right: auto; width: 75%;"/>




```{r}

rm(list=ls())

cfg <- list(
              fileName = './inst/data/Teams.csv',
              rSource = './hw1/Functions.R',
              viewMethod = 'view' # options(render, browser, view)
          )


```




```{r}


knitr::opts_chunk$set(echo = F, 
                      warning = F, 
                      message = F, 
                      eval = T , 
                      results="asis", 
                      fig.height=6, 
                      fig.width=8)

set.seed(1234)


# styling
st_css()

 st_options(
   plain.ascii = FALSE,
   style = 'grid',
   dfSummary.style ='grid',
   freq.silent  = TRUE,
   headings     = FALSE,
   tmp.img.dir  = "./tmp",
   dfSummary.custom.1 =
     expression(
       paste(
         "Q1 - Q3 :",
         round(
           quantile(column_data, probs = .25, type = 2,
                    names = FALSE, na.rm = TRUE), digits = 1
         ), " - ",
         round(
           quantile(column_data, probs = .75, type = 2,
                    names = FALSE, na.rm = TRUE), digits = 1
         )
       )
     )
 )

#source('', local = knitr::knit_global())

```



```{r}

# read data
df <- read.csv('./inst/data/Teams.csv')
df <- sample_n(df, size=200)
statistics <- c('R', 'H', 'X2B', 'X3B', 'HR', 
                    'RA', 'ER', 'HA', 'HRA')


# Preprocess Data
df <- df %>% select('yearID', 'lgID', 'teamID', 'franchID', 'name', 'divID', 'G', 'W', 'L', 
                    'R', 'H', 'X2B', 'X3B', 'HR', 
                    'RA', 'ER', 'HA', 'HRA')

df$wPer <- round(df$W / df$G, 3)
df$pythPer <- (df$R^2) / ((df$R^2) + (df$RA^2))


df <- df %>% mutate(era_cat = case_when(yearID >= 1969 ~ '1969+',
                                                yearID >= 1900 & yearID < 1969 ~ '1900-1969',
                                                yearID < 1900 ~ '1900-'))

df$era_cat <- factor(df$era_cat)



# Split Data
data_split <- initial_split(df, prop = 0.8, strata = era_cat)
train_data <- training(data_split)
test_data  <- testing(data_split)


```



```{r}
train_data %>% 
  skimr::skim()

```



```{r}

print(
  dfSummary(train_data %>% select(-c('teamID','franchID','name')), 
            varnumbers   = TRUE,
            na.col       = TRUE,
            graph.magnif = .8,
            tmp.img.dir  = "/tmp"),
  method = cfg$viewMethod
)


```



```{r}


ggplot(sample_n(train_data, size=100, weight_by = era_cat),
       aes(x = wPer, 
           y = pythPer, 
           group = era_cat, 
           col = era_cat)) + 
  geom_point() + 
  geom_smooth(method = lm, se = FALSE) +
  scale_color_viridis_d(option = "plasma", end = .7)

```




# Approach to Building Models and Execution

In non-opinionated systems, there are several approaches to accomplishing a specific task empowering the user to make their own decisions. This flexibility is diminished in opinionated systems. Opinionated systems have a predefined approach or set of approaches for accomplishing tasks.

The Tidymodel model definition and model execution workflow are detailed below. 



## Recipe

The Tidymodels recipe is similar to the formula definition in the lm() function; however, it allows for feature engineering, variable role definition, and inheritance. The recipe definition provides a programmatically compact methodology for describing a collection of recipes in a single location. 

```{r}


base_recipe <- 
   recipe(W ~ pythPer + era_cat + yearID + teamID + franchID + 
            R + H + X2B + X3B + HR + RA + ER + HA + HRA, data = train_data) %>% 
   update_role(yearID, teamID, franchID, new_role = "ID") %>%
   step_dummy(all_nominal_predictors()) %>% 
   step_zv(all_predictors()) %>% 
   step_normalize(all_predictors())


filter_rec <- 
   base_recipe %>% 
   step_corr(all_of(statistics), threshold = tune())


pca_rec <- 
   base_recipe %>% 
   step_pca(all_of(statistics), num_comp = tune()) %>% 
   step_normalize(all_predictors())

```




# Model Specification

A unified interface to the available model is provided by the parsnip package. This interface decouples the model definition from the semantic details of the underlying package. Users can rapidly experiment with a range of models without getting bogged down in the semantic details of the underlying packages. The level of abstraction from the underlying model reduces the learning curve required to execute different models.


```{r}

# 
# regularized_spec <- 
#    linear_reg(mode = 'regression', penalty = tune(), mixture = tune()) %>% 
#    set_engine("glmnet")
# 
# cart_spec <- 
#    decision_tree(cost_complexity = tune(), min_n = tune()) %>% 
#    set_engine("rpart") %>% 
#    set_mode("regression")
# 
# knn_spec <- 
#    nearest_neighbor(neighbors = tune(), weight_func = tune()) %>% 
#    set_engine("kknn") %>% 
#    set_mode("regression")

lm_model <- 
  linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression")

lms_model <- 
  linear_reg() %>% 
  set_engine("stan") %>% 
  set_mode("regression")

rf_model <- rand_forest(mtry = 10, trees = 2000) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

# pr_model <- poisson_reg(
#   mode = "regression",
#   engine = "poissonreg")




```




# Workflow

The recipe, model specification, pre-processing, and post-processing definitions can be bundled together in a workflow. The workflow package offers coordination and synchronization. 


```{r}

team_wfl_set <- 
   workflow_set(
      preproc = list(simple = base_recipe, 
                     filter = filter_rec, 
                     pca = pca_rec),
      models = list(lm = lm_model, 
                    stan = lms_model,
                    rf = rf_model),
      cross = TRUE
   )
team_wfl_set

```





# Model Selection

```{r}

# team_wfl_set <- 
#    team_wfl_set %>% 
#    anti_join(tibble(wflow_id = c("pca_glmnet", "filter_glmnet")), 
#              by = "wflow_id")

```




```{r}


splits <- 
   vfold_cv(
      train_data,
      v = 10,
      strata = era_cat
   )
splits

```




```{r}

team_wfl_set <- 
   team_wfl_set %>% 
   workflow_map("tune_grid", resamples = splits, grid = 10, 
                metrics = metric_set(mae), verbose = TRUE)

team_wfl_set

```




```{r}

autoplot(team_wfl_set)
autoplot(team_wfl_set, select_best = TRUE)

```



```{r}


rank_results(team_wfl_set, rank_metric = "mae", select_best = TRUE) %>% 
   select(rank, mean, model, wflow_id, .config)


```



# Predict



















## Predict New Values

```{r}

# new_points <- expand.grid(pythPer = .5, 
#                           era_cat = c("1900-", "1900-1969", "1969+"))
# new_points
# 
# 
# 
# mean_pred <- predict(lm_fit, new_data = new_points)
# mean_pred

```






# Model Selection

```{r}

autoplot(team_wfl_set, metric = "mae", id = "filter_rf")


```




# Prediction


```{r}


tmp_wfl <- extract_workflow(team_wfl_set,id = 'simple_rf')



tmp_wfl.fit <- 
  tmp_wfl %>% 
  #finalize_workflow() %>% 
  fit(data = train_data)



tmp_rec <- 
   team_wfl_set %>% 
   extract_workflow_set_result("simple_rf")
tmp_rec



wins_fit <- 
  tmp_wfl %>% 
  fit(data = train_data)


# Make predictions on test set
pred <- tmp_wfl.fit  %>% predict(new_data = test_data)



results <- test_data %>% 
  bind_cols(tmp_wfl.fit %>% 
    predict(new_data = test_data) %>% 
      rename(predictions = .pred))




# t0 <- fit(tmp, data=train_data)
# 
# t1 <- t0 %>% predict(test_data)
# 
# 
# 
# 
# 
# tmp <- team_wfl_set %>% extract_spec_parsnip('simple_glmnet')
# tmp.fit(train_data)
# 
# 
# 
# 
# 
# 
# 
# team_wfl_set %>% modeltime_fit_workflowset(train_data)
# 
# 
# team_wfl_final <- 
#   team_wfl_set %>% 
#   extract_recipe("simple_glmnet")


#team_rec_final.fit()




#wins_results


# wins_results <- 
#   chi_models %>% 
#   extract_workflow_set_result("simple_glmnet")
# wins_results

```


```{r}

# wins_wfl.final <- 
#   chi_models %>% 
#   extract_workflow("simple_glmnet")
# wins_wfl.final

```


```{r}


# wins_workflow_fit <- 
#   wins_wfl.final %>% 
#   #finalize_workflow(tibble(prod_degree = 1)) %>% 
#   fit(data = train_data)
# wins_workflow_fit


```



```{r}

rec <- wins_workflow_fit %>% pull_workflow_prepped_recipe()

df_new_pre_processed <- rec %>%
  bake(new_data = test_data)
 


### Make a prediction for cyl
pred_cyl <- predict(wins_workflow_fit, new_data = test_data, type='numeric')
df_new_pre_processed <- cbind(df_new_pre_processed, pred_cyl)
df_new_pre_processed








```




# Conclusion


Conclusion
 
It has been a long argument, so perhaps a summary of the main points will help convince you that tidymodels should go back to the drawing board.
 
1. tidymodels appears to be based on the idea that modelling should be made easier, while a better approach would have been to base it on a comprehensive design that captures how modelling ought to be performed within the tidyverse. 

2. tidymodels duplicates some features already available in the tidyverse.

3. tidymodels uses separate functions to define the stages in the analysis and only performs the calculations once the full method is specified. This has two consequences,
it discourages checking of intermediate steps and so makes errors more likely.
it strays away from the basic structure of R in which a function is used to define an action that is to be performed at some later stage and the arguments to that function define the specific choices.

4. tidymodels makes it easier to specify the model that you want to use by hiding the details and terminology of the package that will perform the calculations. In doing so,
tidymodels makes decisions for the user in the form of defaults, so that the user will find it harder to keep track of the details of their analysis.
the user is encouraged to use methods that they do not completely understand.
5. tidymodels offers a complete one-stop solution to modelling. Consequently,
it is difficult for the user to adapt the tidymodels functions for use outside of tidymodels.
it is difficult for users to add to tidymodels.
the design is centrally controlled, which will limit the speed of its development and will not make full use of the skills of the R Community.
I believe that if the tidymodels team were to start afresh, they would come up with a design that would be very similar to that used in the modelr package. tidymodels needs to provide a strong design and a few basic tools. If the design and tools are any good, the community of R users will adopt them and develop new packages for data modelling.



https://staffblogs.le.ac.uk/teachingr/2020/10/05/on-not-using-tidymodels/




Overlays an workflow and methodology for solving problem - Since the frameworks are built by experience individuals in a particular field it also includes their best practices around how to approach problems. 
The tools help guid individuals into the right directions













```{r}


base_recipe <- 
   recipe(W ~ pythPer + era_cat + yearID + teamID + franchID + 
            R + H + X2B + X3B + HR + RA + ER + HA + HRA, data = train_data) %>% 
   update_role(yearID, teamID, franchID, new_role = "ID") %>%
   step_dummy(all_nominal_predictors()) %>% 
   step_zv(all_predictors()) %>% 
   step_normalize(all_predictors())


lm_model <- 
  linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression")


lm_wflow <- 
  workflow() %>% 
  add_recipe(base_recipe) %>%
  add_model(lm_model) 

  
wins_fit <- 
  lm_wflow %>% 
  fit(data = train_data)


# Make predictions on test set
pred <- wins_fit %>% predict(new_data = test_data)


# Predict rentals for the test set and bind it to the test_set
results <- test_data %>% 
  bind_cols(wins_fit %>% 
    predict(new_data = test_data) %>% 
      rename(predictions = .pred))


# Compare predictions
results %>% 
  select(c(W, predictions)) %>% 
  slice_head(n = 10)



```






```{r}
library(DALEXtra)

explain_tidymodels(wins_fit, data=train_data, y=train_data$W)




```





```{r}

library(rstanarm)
library(ranger)
library(poissonreg)

base_recipe <- 
   recipe(W ~ pythPer + era_cat + yearID + teamID + franchID + 
            R + H + X2B + X3B + HR + RA + ER + HA + HRA, data = train_data) %>% 
   update_role(yearID, teamID, franchID, new_role = "ID") %>%
   step_dummy(all_nominal_predictors()) %>% 
   step_zv(all_predictors()) %>% 
   step_normalize(all_predictors())


lm_model <- 
  linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression")

lms_model <- 
  linear_reg() %>% 
  set_engine("stan") %>% 
  set_mode("regression")

rf_model <- rand_forest(mtry = 10, trees = 2000) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

pr_model <- poisson_reg(
  mode = "regression",
  #penalty = NULL,
  #mixture = NULL,
  engine = "glm"
)



lm_wflow <- 
  workflow() %>% 
  add_recipe(base_recipe) %>%
  add_model(lms_model) 

  
wins_fit <- 
  lm_wflow %>% 
  fit(data = train_data)


# Make predictions on test set
pred <- wins_fit %>% predict(new_data = test_data)


# Predict rentals for the test set and bind it to the test_set
results <- test_data %>% 
  bind_cols(wins_fit %>% 
    predict(new_data = test_data) %>% 
      rename(predictions = .pred))


# Compare predictions
results %>% 
  select(c(W, predictions)) %>% 
  slice_head(n = 10)



```



